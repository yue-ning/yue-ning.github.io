<!DOCTYPE html>
<html lang="en">
<head>
    <title>Yue Ning: Fall 2019 Natural Language Processing</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="blog, accent, , Yue Ning, jekyll">
    <meta name="author" content="">
    
    
    
    <meta name="description" content="">
    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <link rel="alternate" type="application/rss+xml" title="Yue Ning RSS" href="/feed.xml" />
    <link rel="stylesheet" href="./css/main.css">
    
    
    <!-- Facebook Open Graph -->
    <meta name="og:description" content="">
    <meta name="og:title" content="Yue Ning">
    <meta name="og:url" content="./research.html">
    <meta name="og:type" content="article">
    
    
    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Yue Ning">
    <meta name="twitter:description" content="">
    <meta name="twitter:url" content="./research.html">
    
        <meta name="twitter:image" content="">
    
</head>
<body>
    <div class="wrapper">
        <div class="navbar" style="max-width:1000px;margin:auto">
            <a id="author-name" class="alignable pull-left">CS584: Natural Language Processing </a>
            <ul class="alignable pull-right navbar-ul">
                <li class="alignable pull-right nav-list"><a class="cool-link" href=".">&#8594;home</a></li>
            </ul>
        </div>
        <div style="clear:both"></div>
        <hr>
       <div class="content" style="max-width:1000px;margin:auto">
         <p></p>

<h2>General Information</h2>

<ul>
  <li><strong>Time</strong>: Wednesday 6:15PM - 8:45PM, fall semester, 2019</li>
  <li><strong>Location</strong>: TBD</li>
  <li><strong>Instructor</strong>: Yue Ning </li>
  <li><strong>Office Hours</strong>: TBD </li>
  <li><strong>Teaching assistant</strong>: TBD </li>
  <li><strong>Course details</strong>: We will be using <a href="">Canvas</a> for online discussion, announcements, and homework submission. You are encouraged to ask and answer questions on the forum as long as you do not give away solutions to homework problems. Participation in the online forum will count towards class participation.</li>
</ul>




<h2>What is this course about?</h2>

Natural language processing (NLP) is one of the most important technologies in the era of information. Comprehending human language is also a crucial and challenging part of artificial intelligence. People communicate almost everything in language: conferences, emails, customer service, language translation, web searches, reports, etc. There are a large variety of underlying tasks and machine learning models behind NLP applications. Recently, deep learning approaches have achieved high performance in many different NLP tasks. Instead of traditional and task-specific feature engineering, deep learning can solve tasks with single end-to-end models. The course provides an introduction to machine learning research applied to NLP. We will cover topics including word vector representations, neural networks, recurrent neural networks, convolutional neural networks, semi-supervised models, as well as some attention-based models. 

<h2>Prerequisites</h2>
This course is fast-paced and covers a lot of ground, so it is important that you have a solid foundation on both the theoretical and empirical fronts. <strong><u>You should have background in python programming, probability theory, linear algebra, calculus, and foundations of machine learning.
</u></strong>

<h2>Reading</h2>
<ul>

  <li>Ian Goodfellow and Yoshua Bengio and Aaron Courville, 2016. <a href="https://www.deeplearningbook.org/">Deep Learning</a> (<strong>DL</strong>), MIT Press. We will cover topics including basic neural networks, back propagation, and CNN.</li>
  <li> Dan Jurafsky and James H. Martin. 2018.
  <a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">Speech and Language Processing (3rd ed. draft)</a> (<strong>SLP</strong>).</li>
  <li>Yoav Goldberg. <a href="https://u.cs.biu.ac.il/~yogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</a> (<strong>NNLP</strong>).</li>
</ul>

<h2>Coursework</h2>
Submissions: All assignments (homework problems and project milestones) must be submitted on Canvas by 6:30 PM. You have two (2) late days in total that can be distributed among the homework assignments without penalty.
<ul>
  <li>Homework (50%): There will be bi-weekly homework assignments with both written and programming parts. Each assignment is centered around an application and will also deepen your understanding of the theoretical concepts</li>
    <li>Midterm Exam (15%): The midterm exam is to evaluate your understanding of the course so far. It will be an in-class written exam.</li>
      <li>Project (25%): The final project provides an opportunity for you to use the tools from class to build something interesting of your choice. You need to make a presentation in class and submit a report. </li>
    <li>Participation (5%): Discussions on Canvas and attending classes 
      <li>Quizzes (5%): top 2 out of 5 quizzes</li>

</ul>
<h2>Schedule (tentative)</h2>
<table border="1">
              <tr ALIGH=LEFT>
                 <th VALIGN=TOP WIDTH=80px ALIGN=center>
                Week
                </th>
              <th VALIGN=TOP WIDTH=100px ALIGN=left>
                Date
                </th>
              <th VALIGN=TOP WIDTH=240px  ALIGN=left>
               Topic
              </th>
               <th VALIGN=TOP WIDTH=140px ALIGN=left>
                Reading
                </th>
                  <th VALIGN=TOP WIDTH=120px ALIGN=left>
                Event
                </th>
            </tr>
             
              <tr ALIGH=LEFT>
               <td VALIGN=TOP WIDTH=80px ALIGN=center>
                1
                </td>
               <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Aug. 28
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Introduction to NLP
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                  CH1-3 SLP
                </td>
                <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
             <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                2
                </td>
            <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Sep. 4
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Machine learning basics & Neural networks
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    CH2-5 DL
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
            <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                3
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Sep. 11
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Vector Semantics: TFIDF, CBOW, Skip-gram, Glove
              </td>
                  <td VALIGN=TOP WIDTH=320px ALIGN=left>
                    CH 6 SLP 
                    <br>
                    <a href="https://arxiv.org/pdf/1301.3781.pdf">Mikolov et al. 2013</a> (word2vec)<br>
                <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al. 2013</a> (Negative sampling)
                </td>
                 <td VALIGN=TOP WIDTH=50px ALIGN=left>
                  
                </td>
            </tr>
            <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                4
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Sep. 18
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Deep feedforward networks
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                CH6-8 DL
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>

                </td>
            </tr>
            <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                5
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Sep. 25
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
              Language Modeling
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    CH 3 SLP
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                  
                </td>
            </tr>
             <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                6
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Oct. 2
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
              Dependency parsing and Recurrent neural networks(RNN)
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    CH 9, 13 SLP
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
              <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                7
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Oct. 9
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
             More on RNNs (vanishing gradients)
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    <a href="http://www.deeplearningbook.org/contents/rnn.html">CH10 DL</a>
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
            <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                8
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Oct. 16
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
                <strong>Midterm exam</strong>
               </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
              <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                9
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
               Oct. 23
                </td>
                 <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
              Convolutional neural networks
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    CH9 DL
                </td>
             
                  <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
             <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                10
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Oct. 30
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Machine Translation, Seq2seq models, attention models
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                  CH 10 DL <br>
                  <a href="https://arxiv.org/pdf/1409.3215.pdf">Sutskever et al. 2014</a>
                </td>
                  <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
           <tr ALIGH=LEFT>
              <td VALIGN=TOP WIDTH=80px ALIGN=center>
                11
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Nov. 6
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Tree recursive neural networks and constituency parsing
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                    <a href="https://www.aclweb.org/anthology/P13-1045">Socher et al. 2013</a>
                <br>
                <a href="https://arxiv.org/pdf/1805.01052.pdf">Kitaev et al. 2018</a>
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
             <tr ALIGH=LEFT>  
              <td VALIGN=TOP WIDTH=80px ALIGN=center>
                12
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
                Nov. 13
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Advanced architectures and memory networks
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
               
                </td>
                <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
             <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                13
                </td>
            <td VALIGN=TOP WIDTH=100px ALIGN=left>
              Nov. 20
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               Semi-supervised learning for NLP
              </td>
              <td VALIGN=TOP WIDTH=140px ALIGN=left>
                -
                </td>
              <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>

            </tr>
             <tr ALIGH=LEFT>
                <td VALIGN=TOP WIDTH=80px ALIGN=center>
                14
                </td>
                 <td VALIGN=TOP WIDTH=100px ALIGN=left>
                <font color="green">Nov. 27 </font>
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
             <font color="green"> Thanksgiving - no class</font>
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                -
                </td>
            
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
              <tr ALIGH=LEFT>
                  <td VALIGN=TOP WIDTH=80px ALIGN=center>
                15
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
              Dec. 4
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
               
               <strong>Project presentation</strong>
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
               -
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                </td>
            </tr>
              <tr ALIGH=LEFT>
                  <td VALIGN=TOP WIDTH=80px ALIGN=center>
                16
                </td>
             <td VALIGN=TOP WIDTH=100px ALIGN=left>
              Dec. 11
                </td>
              <td VALIGN=TOP WIDTH=240px  ALIGN=LEFT>
                <strong>Project presentation</strong>
              </td>
                  <td VALIGN=TOP WIDTH=140px ALIGN=left>
                -
                </td>
                 <td VALIGN=TOP WIDTH=120px ALIGN=left>
                  <strong> Project report <font color="red">due</font></strong>
                </td>
            </tr>
            
  </table>


    </div>
 </div>

    <br><br><br>
    <hr>
</body>
